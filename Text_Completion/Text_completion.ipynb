{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kIyE_FJc4K6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text and remove stopwords\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
        "    return words\n",
        "\n",
        "def build_markov_chain(words, chain_length):\n",
        "    markov_chain = {}\n",
        "    for i in range(len(words) - chain_length):\n",
        "        current_words = tuple(words[i:i+chain_length])\n",
        "        next_word = words[i + chain_length]\n",
        "        if current_words in markov_chain:\n",
        "            markov_chain[current_words].append(next_word)\n",
        "        else:\n",
        "            markov_chain[current_words] = [next_word]\n",
        "    return markov_chain\n",
        "\n",
        "def generate_sentence(markov_chain, start_words, num_generated):\n",
        "    current_words = tuple(start_words)\n",
        "    generated_sentence = list(start_words)\n",
        "    for _ in range(num_generated):\n",
        "        if current_words in markov_chain:\n",
        "            next_word = random.choice(markov_chain[current_words])\n",
        "            generated_sentence.append(next_word)\n",
        "            current_words = tuple(generated_sentence[-len(start_words):])\n",
        "        else:\n",
        "            break\n",
        "    return ' '.join(generated_sentence)\n",
        "\n",
        "def generate(filename: str, start_words: list[str], chain_length: int, num_generated: int) -> str:\n",
        "    text = read_file(filename)\n",
        "    words = preprocess_text(text)\n",
        "    markov_chain = build_markov_chain(words, chain_length)\n",
        "    return generate_sentence(markov_chain, start_words, num_generated)\n",
        "\n",
        "# Example usage:\n",
        "filename = \"your_filename.txt\"  # Provide the filename\n",
        "start_words = [\"start\", \"words\"]  # Provide start words\n",
        "chain_length = len(start_words)\n",
        "num_generated = 50  # Number of words to generate\n",
        "generated_text = generate(filename, start_words, chain_length, num_generated)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34xuHCs7dCWM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}